{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.github_helper import get_readme, get_rate_limit_and_wait_time, get_starcount\n",
    "import pandas as pd\n",
    "from util.file_helper import write_text_to_file, to_valid_file_name\n",
    "from urllib.parse import urlparse\n",
    "import cmarkgfm\n",
    "from tqdm import tqdm\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/acl/scraping/papers_with_code.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 788 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 9100/9300 [2:10:06<02:11,  1.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 183 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9300/9300 [2:15:34<00:00,  1.14it/s]  \n"
     ]
    }
   ],
   "source": [
    "# In case of error, use this to resume\n",
    "# df = df.loc[5704:]\n",
    "repo_file_map = {}\n",
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    parsed_url = urlparse(row[\"code\"])\n",
    "    df.loc[idx, \"platform\"] = parsed_url.netloc\n",
    "    df.loc[idx, \"repo_dir\"] = parsed_url.path[1:]\n",
    "    if parsed_url.netloc == \"github.com\":\n",
    "        if idx % 100 == 0:\n",
    "            remaining, wait_time = get_rate_limit_and_wait_time()\n",
    "            if (remaining < 100):\n",
    "                print(f\"Waiting for {wait_time} seconds\")\n",
    "                time.sleep(wait_time)\n",
    "        readme_content, url = get_readme(parsed_url.path[1:])\n",
    "        if readme_content:\n",
    "            df.loc[idx, \"stars\"] = get_starcount(parsed_url.path[1:])\n",
    "            df.loc[idx, \"readme_url_path\"] = url\n",
    "            filename = to_valid_file_name(row[\"paper\"])\n",
    "            write_text_to_file(\n",
    "                readme_content, \"data/acl/readme/md/\", f\"{filename}.md\")\n",
    "            html = cmarkgfm.github_flavored_markdown_to_html(readme_content)\n",
    "            write_text_to_file(\n",
    "                html, \"data/acl/readme/html/\", f\"{filename}.html\")\n",
    "            repo_file_map[parsed_url.path[1:]] = filename\n",
    "df.to_csv(\"data/acl/scraping/papers_with_code_readme.csv\", index=False)\n",
    "filemap_df = pd.DataFrame(list(repo_file_map.items()),\n",
    "                          columns=['repo', 'filename'])\n",
    "filemap_df.to_csv(\"data/acl/readme/repo_file_map.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurips_df = pd.read_csv(\"data/paperswithcode/neurips2019_checklist.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_file_map = {}\n",
    "for idx, row in tqdm(neurips_df.iterrows(), total=neurips_df.shape[0]):\n",
    "    parsed_url = urlparse(row[\"url\"])\n",
    "    if parsed_url.netloc == \"github.com\":\n",
    "        readme_content, url = get_readme(parsed_url.path[1:])\n",
    "        if readme_content:\n",
    "            filename = to_valid_file_name(parsed_url.path[1:])\n",
    "            write_text_to_file(\n",
    "                readme_content, \"data/paperswithcode/readme/md/\", f\"{filename}.md\")\n",
    "            html = cmarkgfm.github_flavored_markdown_to_html(readme_content)\n",
    "            write_text_to_file(\n",
    "                html, \"data/paperswithcode/readme/html/\", f\"{filename}.html\")\n",
    "            repo_file_map[parsed_url.path[1:]] = filename\n",
    "filemap_df = pd.DataFrame(list(repo_file_map.items()),\n",
    "                          columns=['repo', 'filename'])\n",
    "filemap_df.to_csv(\"data/paperswithcode/readme/repo_file_map.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
