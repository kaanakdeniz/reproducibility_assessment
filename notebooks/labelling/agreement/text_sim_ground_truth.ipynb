{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from util.file_helper import read_all_csv_files\n",
    "from util.data_structure_helper import dict_to_df\n",
    "from util.annotator_helper import (\n",
    "    form_label_dataframes,\n",
    "    get_agreement,\n",
    "    fix_multiple_labels,\n",
    "    select_label_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['kappa', 'alpha', 'avg_Ao', 'multi_kappa', 'S', 'pi']\n",
    "labellings = read_all_csv_files(\n",
    "    'data/acl/sections/labelled/manuel/text_sim_header/raw/')\n",
    "dfs_dict = select_label_columns(labellings)\n",
    "annotators = sorted(dfs_dict.keys())\n",
    "selected_metric = \"weighted_kappa\"\n",
    "dfs = dict_to_df(dfs_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement with multilabels: 0.45\n",
      "Agreement without multilabels: 0.587\n",
      "Number of records with multiple labels: 159\n",
      "Number of records with no intersection: 96\n",
      "Number of records with intersection: 63\n"
     ]
    }
   ],
   "source": [
    "multilabel_data = form_label_dataframes(dfs)\n",
    "print(\n",
    "    f\"Agreement with multilabels: {round(get_agreement(multilabel_data, selected_metric),3)}\")\n",
    "\n",
    "\n",
    "label_fixed_df, not_intersected_count, intersection_counts, number_of_multi_labels = fix_multiple_labels(\n",
    "    dfs)\n",
    "data = form_label_dataframes(label_fixed_df)\n",
    "print(\n",
    "    f\"Agreement without multilabels: {round(get_agreement(data, selected_metric),3)}\")\n",
    "print(f\"Number of records with multiple labels: {number_of_multi_labels}\")\n",
    "print(f\"Number of records with no intersection: {not_intersected_count}\")\n",
    "print(\n",
    "    f\"Number of records with intersection: {number_of_multi_labels - not_intersected_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'requirements': 13,\n",
       " 'evaluation': 25,\n",
       " 'training': 14,\n",
       " 'pre-trained models': 6,\n",
       " 'results': 2,\n",
       " 'introduction': 3}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-labelled Human Agreement Range Statistics\n",
      "Score range: 0 - Agreement: 0.587 - Number of records: 1050\n",
      "Score range: 0.1 - Agreement: 0.588 - Number of records: 1045\n",
      "Score range: 0.2 - Agreement: 0.591 - Number of records: 1000\n",
      "Score range: 0.3 - Agreement: 0.602 - Number of records: 835\n",
      "Score range: 0.4 - Agreement: 0.603 - Number of records: 603\n",
      "Score range: 0.5 - Agreement: 0.647 - Number of records: 342\n",
      "Score range: 0.6 - Agreement: 0.702 - Number of records: 128\n",
      "Score range: 0.7 - Agreement: 0.745 - Number of records: 59\n",
      "Score range: 0.8 - Agreement: 0.813 - Number of records: 21\n",
      "Score range: 0.9 - Agreement: 1.0 - Number of records: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Single-labelled Human Agreement Range Statistics\")\n",
    "score_ranges = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for score_range in score_ranges:\n",
    "    frames = select_label_columns(labellings, score_range)\n",
    "    frames = dict_to_df(frames)\n",
    "    ranged_label_fixed, _, _, _ = fix_multiple_labels(frames)\n",
    "    data = form_label_dataframes(ranged_label_fixed)\n",
    "    print(f\"Score range: {score_range} - Agreement: {round(get_agreement(data, selected_metric), 3)} - Number of records: {len(ranged_label_fixed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreed label count: 580\n",
      "Disagreed label count: 470\n"
     ]
    }
   ],
   "source": [
    "agreed_labels = label_fixed_df[label_fixed_df.eq(\n",
    "    label_fixed_df.iloc[:, 0], axis=0).all(1)]\n",
    "agreed_label_count = len(agreed_labels)\n",
    "disagreed_label_count = len(label_fixed_df) - agreed_label_count\n",
    "print(f\"Agreed label count: {agreed_label_count}\")\n",
    "print(f\"Disagreed label count: {disagreed_label_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'requirements': 181,\n",
       " 'irrelevant': 111,\n",
       " 'evaluation': 95,\n",
       " 'training': 85,\n",
       " 'introduction': 75,\n",
       " 'pre-trained models': 22,\n",
       " 'results': 11}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreed_labels[\"annotator1\"].value_counts().to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disagreed label count: 470\n",
      "Partially agreed label count: 355\n",
      "Partially disagreed label count: 115\n"
     ]
    }
   ],
   "source": [
    "disagreed_df = label_fixed_df.drop(agreed_labels.index)\n",
    "disagreed_df_cp = disagreed_df.copy()\n",
    "for _, row in disagreed_df.iterrows():\n",
    "    labels = row.values\n",
    "    common_label, count = Counter(labels).most_common(1)[0]\n",
    "    if count == 2:\n",
    "        disagreed_df_cp.loc[row.name, \"partially_agreed_lbl\"] = common_label\n",
    "    else:\n",
    "        disagreed_df_cp.loc[row.name, \"partially_agreed_lbl\"] = \"-\"\n",
    "\n",
    "print(f\"Disagreed label count: {len(disagreed_df_cp)}\")\n",
    "print(\n",
    "    f\"Partially agreed label count: {len(disagreed_df_cp[disagreed_df_cp['partially_agreed_lbl'] != '-'])}\")\n",
    "print(\n",
    "    f\"Partially disagreed label count: {len(disagreed_df_cp[disagreed_df_cp['partially_agreed_lbl'] == '-'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of partial labels: 935\n"
     ]
    }
   ],
   "source": [
    "partially_agreed_labels = disagreed_df_cp[disagreed_df_cp['partially_agreed_lbl'] != '-']\n",
    "agreed_labels.assign(partially_agreed_lbl=agreed_labels[\"annotator1\"])\n",
    "label_fixed_df[\"human\"] = \"\"\n",
    "label_fixed_df.loc[agreed_labels.index, \"human\"] = agreed_labels[\"annotator1\"]\n",
    "label_fixed_df.loc[partially_agreed_labels.index,\n",
    "                   \"human\"] = partially_agreed_labels[\"partially_agreed_lbl\"]\n",
    "partial_labels = label_fixed_df[label_fixed_df[\"human\"] != \"\"][[\"human\"]]\n",
    "partial_labels.loc[:,\n",
    "                   \"text\"] = labellings[\"annotator1\"].loc[partial_labels.index].text\n",
    "print(f\"Lenght of partial labels: {len(partial_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of irrelevants: 204\n",
      "Number of not irrelevants: 731\n"
     ]
    }
   ],
   "source": [
    "irrelevants = partial_labels[partial_labels.human == \"irrelevant\"]\n",
    "not_irrelevants = partial_labels.loc[partial_labels.index.difference(\n",
    "    irrelevants.index)]\n",
    "print(f\"Number of irrelevants: {len(irrelevants)}\")\n",
    "print(f\"Number of not irrelevants: {len(not_irrelevants)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index finding\n",
    "\n",
    "not_irrelevants.sort_values(by=['text'], inplace=True)\n",
    "not_irrelevants.human.replace(\n",
    "    to_replace='pre-trained models', value=\"pretrained_model\", inplace=True)\n",
    "df = pd.read_csv(\"data/acl/sections/sections_header_clean.csv\")\n",
    "df['parent_header'] = df['parent_header'].fillna(\"\")\n",
    "df[\"text\"] = df[[\"parent_header\", \"header\", \"content\"]].agg(\"\\n\".join, axis=1)\n",
    "df[\"text\"] = df[\"text\"].str.removeprefix(\"\\n\")\n",
    "indexes = df[df.text.isin(not_irrelevants.text)].sort_values(by=\"text\").index\n",
    "not_irrelevants[\"data_index\"] = indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv(\"data/acl/sections/sections_clean.csv\", index_col=0)\n",
    "not_irrelevants.drop(indexes.difference(df_clean.index), inplace=True)\n",
    "not_irrelevants.set_index(\"data_index\", inplace=True)\n",
    "# not_irrelevants.to_csv(\n",
    "#     \"data/acl/sections/labelled/manuel/text_sim_header/processed/partially_agreed_non_irrelevants.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f60512b77569a1fe8fa6afbbf890ccf92fee607013e721db6d6fd4d6de4a65f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
