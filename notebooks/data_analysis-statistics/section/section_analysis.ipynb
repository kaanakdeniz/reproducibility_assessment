{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = pd.read_csv(\"data/acl/sections/sections.csv\")\n",
    "unmasked = pd.read_csv(\"data/acl/sections/sections_unmasked.csv\")\n",
    "grouped_masked = pd.read_csv(\"data/acl/sections/sections_grouped.csv\")\n",
    "grouped_unmasked = pd.read_csv(\n",
    "    \"data/acl/sections/sections_grouped_unmasked.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Uzunluk Analizi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_describe(df, typename):\n",
    "    df = df.copy()\n",
    "    df[\"tokens\"] = df[\"content\"].astype(str).progress_apply(word_tokenize)\n",
    "    df[\"header_tokens\"] = df[\"header\"].astype(\n",
    "        str).progress_apply(word_tokenize)\n",
    "    if \"parent_header\" in df.columns:\n",
    "        df[\"parent_header_tokens\"] = df[\"parent_header\"].astype(\n",
    "            str).progress_apply(word_tokenize)\n",
    "    # df[\"token_count\"] = df[\"tokens\"].apply(len)\n",
    "    df[\"token_count\"] = df[\"tokens\"].progress_apply(len)\n",
    "    df[\"header_token_count\"] = df[\"header_tokens\"].progress_apply(len)\n",
    "    if \"parent_header\" in df.columns:\n",
    "        df[\"parent_header_token_count\"] = df[\"parent_header_tokens\"].progress_apply(\n",
    "            len)\n",
    "    df[\"total_token_count\"] = df[\"token_count\"] + df[\"header_token_count\"]\n",
    "    if \"parent_header\" in df.columns:\n",
    "        df[\"total_token_count\"] += df[\"parent_header_token_count\"]\n",
    "\n",
    "    desc = df[[\"total_token_count\"]].describe()\n",
    "    desc.rename(columns={\"total_token_count\": typename}, inplace=True)\n",
    "    return df, desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_df, masked_desc = get_token_describe(masked, \"masked\")\n",
    "unmasked_df, unmasked_desc = get_token_describe(unmasked, \"unmasked\")\n",
    "grouped_masked_df, grouped_masked_desc = get_token_describe(\n",
    "    grouped_masked, \"grouped_masked\")\n",
    "grouped_unmasked_df, grouped_unmasked_desc = get_token_describe(\n",
    "    grouped_unmasked, \"grouped_unmasked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = pd.concat([masked_desc, unmasked_desc,\n",
    "                     grouped_masked_desc, grouped_unmasked_desc], axis=1).T\n",
    "analysis.astype(int).to_latex(\"x.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_token_count(df, limit):\n",
    "    count = df[df[\"token_count\"] > limit].shape[0]\n",
    "    return count, round(count / df.shape[0], 4) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Masked sections(>512 tokens):\", filter_by_token_count(masked_df, 512))\n",
    "print(\"Unmasked sections(>512 tokens):\",\n",
    "      filter_by_token_count(unmasked_df, 512))\n",
    "print(\"Grouped masked sections(>512 tokens):\",\n",
    "      filter_by_token_count(grouped_masked_df, 512))\n",
    "print(\"Grouped unmasked sections(>512 tokens):\",\n",
    "      filter_by_token_count(grouped_unmasked_df, 512))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maskeleme Analizi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.readme_parser import encodings\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_word(text):\n",
    "    return re.findall(r\"(<\\w+)\", text)\n",
    "\n",
    "\n",
    "encodings = {key.replace(\"_c\", \"\"): extract_word(\n",
    "    encoding)[0] for key, encoding in encodings.items()}\n",
    "masked_cp = masked.copy()\n",
    "for mask in encodings:\n",
    "    masked_cp[mask] = masked_cp[\"content\"].str.count(encodings[mask])\n",
    "masked_cp[\"mask_total\"] = masked_cp[encodings.keys()].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_repo = masked_cp.groupby(\"repo\").sum(numeric_only=True).reset_index()\n",
    "group_by_repo.describe().iloc[1:, 2:].T\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
